<!DOCTYPE html>
<html>
<head hexo-theme='https://volantis.js.org/#'>
  <meta charset="utf-8">
  <!-- SEO相关 -->
  
    
  
  <!-- 渲染优化 -->
  <meta name="renderer" content="webkit">
  <meta name="force-rendering" content="webkit">
  <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
  <meta name="HandheldFriendly" content="True" >
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <!-- 页面元数据 -->
  
    <title>Attention机制简单总结 - Ziv（瓶子）</title>
  
    <meta name="keywords" content="Artificial Intelligence">
  
  
    <meta name="description" content="Attention机制简单总结">
  

  <!-- feed -->
  
    <link rel="alternate" href="/atom.xml" title="Ziv（瓶子）">
  

  <!-- import meta -->
  

  <!-- link -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.13/css/all.min.css">
  
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css">

  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-waves@0.7.6/dist/waves.min.css">

  

  

  
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.css">
  

  

  <!-- import link -->
  

  
  
    
<link rel="stylesheet" href="/css/style.css">

  

  <script>
    function setLoadingBarProgress(num) {
      document.getElementById('loading-bar').style.width=num+"%";
    }
  </script>

  
  
</head>

<body>
  
  <div id="loading-bar-wrapper">
  <div id="loading-bar"></div>
</div>
<header class="l_header shadow">
  <div class='container'>
  <div class='wrapper'>
    <div class='nav-sub'>
      <p class="title"></p>
      <ul class='switcher nav-list-h'>
        <li><a class="s-comment fas fa-comments fa-fw" target="_self" href='javascript:void(0)'></a></li>
        
          <li><a class="s-toc fas fa-list fa-fw" target="_self" href='javascript:void(0)'></a></li>
        
      </ul>
    </div>
		<div class="nav-main">
      
        
        <a class="title flat-box" target="_self" href='/'>
          
          
          
          
            ZIV <b><sup style='color:#3AA757'></sup></b>
          
        </a>
      

			<div class='menu navigation'>
				<ul class='nav-list-h'>
          
          
          
            
            
              <li>
                <a class="flat-box" href=/
                  
                  
                  
                    id="home"
                  >
                  <i class='fas fa-rss fa-fw'></i>博客
                </a>
                
              </li>
            
          
          
            
            
              <li>
                <a class="flat-box" href=/categories/
                  
                  
                  
                    id="categories"
                  >
                  <i class='fas fa-folder-open fa-fw'></i>分类
                </a>
                
              </li>
            
          
          
            
            
              <li>
                <a class="flat-box" href=/tags/
                  
                  
                  
                    id="tags"
                  >
                  <i class='fas fa-tags fa-fw'></i>标签
                </a>
                
              </li>
            
          
          
            
            
              <li>
                <a class="flat-box" href=/archives/
                  
                  
                  
                    id="archives"
                  >
                  <i class='fas fa-archive fa-fw'></i>归档
                </a>
                
              </li>
            
          
          
            
            
              <li>
                <a class="flat-box" href=/books/
                  
                  
                  
                    id="books"
                  >
                  <i class='fas fa-book fa-fw'></i>瓶·书
                </a>
                
              </li>
            
          
          
            
            
              <li>
                <a class="flat-box" href=/movies/
                  
                  
                  
                    id="movies"
                  >
                  <i class='fas fa-play-circle fa-fw'></i>瓶·影
                </a>
                
              </li>
            
          
          
            
            
              <li>
                <a class="flat-box" href=/friends/
                  
                  
                  
                    id="friends"
                  >
                  <i class='fas fa-link fa-fw'></i>友链
                </a>
                
              </li>
            
          
          
            
            
              <li>
                <a class="flat-box" href=/about/
                  
                  
                  
                    id="about"
                  >
                  <i class='fas fa-info-circle fa-fw'></i>关于
                </a>
                
              </li>
            
          
          
				</ul>
			</div>

      <div class="m_search">
        <form name="searchform" class="form u-search-form">
          <i class="icon fas fa-search fa-fw"></i>
          <input type="text" class="input u-search-input" placeholder="Search..." />
        </form>
      </div>

			<ul class='switcher nav-list-h'>
				
					<li><a class="s-search fas fa-search fa-fw" target="_self" href='javascript:void(0)'></a></li>
				
				<li>
          <a class="s-menu fas fa-bars fa-fw" target="_self" href='javascript:void(0)'></a>
          <ul class="menu-phone list-v navigation white-box">
            
              
            
              <li>
                <a class="flat-box" href=/
                  
                  
                  
                    id="home"
                  >
                  <i class='fas fa-rss fa-fw'></i>博客
                </a>
                
              </li>
            
          
            
              
            
              <li>
                <a class="flat-box" href=/categories/
                  
                  
                  
                    id="categories"
                  >
                  <i class='fas fa-folder-open fa-fw'></i>分类
                </a>
                
              </li>
            
          
            
              
            
              <li>
                <a class="flat-box" href=/tags/
                  
                  
                  
                    id="tags"
                  >
                  <i class='fas fa-tags fa-fw'></i>标签
                </a>
                
              </li>
            
          
            
              
            
              <li>
                <a class="flat-box" href=/archives/
                  
                  
                  
                    id="archives"
                  >
                  <i class='fas fa-archive fa-fw'></i>归档
                </a>
                
              </li>
            
          
            
              
            
              <li>
                <a class="flat-box" href=/books/
                  
                  
                  
                    id="books"
                  >
                  <i class='fas fa-book fa-fw'></i>瓶·书
                </a>
                
              </li>
            
          
            
              
            
              <li>
                <a class="flat-box" href=/movies/
                  
                  
                  
                    id="movies"
                  >
                  <i class='fas fa-play-circle fa-fw'></i>瓶·影
                </a>
                
              </li>
            
          
            
              
            
              <li>
                <a class="flat-box" href=/friends/
                  
                  
                  
                    id="friends"
                  >
                  <i class='fas fa-link fa-fw'></i>友链
                </a>
                
              </li>
            
          
            
              
            
              <li>
                <a class="flat-box" href=/about/
                  
                  
                  
                    id="about"
                  >
                  <i class='fas fa-info-circle fa-fw'></i>关于
                </a>
                
              </li>
            
          
            
          </ul>
        </li>
			</ul>
		</div>
	</div>
  </div>
</header>

<script>setLoadingBarProgress(40);</script>



  <div class="l_body nocover">
    <div class='body-wrapper'>
      

<div class='l_main'>
  

  
    <article id="post" class="post white-box reveal floatable article-type-post" itemscope itemprop="blogPost">
      


  <section class='meta'>
    
      
      
      <div class="meta" id="header-meta">
        
          
  <h1 class="title">
    <a href="/passages/attention-ji-zhi-jian-yao-zong-jie/">
      Attention机制简单总结
    </a>
  </h1>


        
        <div class='new-meta-box'>
          
            
          
            
              
<div class='new-meta-item author'>
  <a href="http://ziv_blog.top" target="_blank" rel="nofollow noopener">
    <img src="https://github.com/cn-Wziv/image_repo/blob/master/%E6%B4%AA%E7%8C%AB.jpg">
    <p>瓶子</p>
  </a>
</div>

            
          
            
              
  
  <div class='new-meta-item category'>
    <a href='/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/' rel="nofollow">
      <i class="fas fa-folder-open fa-fw" aria-hidden="true"></i>
      <p>深度学习</p>
    </a>
  </div>


            
          
            
              <div class="new-meta-item date">
  <a class='notlink'>
    <i class="fas fa-calendar-alt fa-fw" aria-hidden="true"></i>
    <p>发布于：Oct 21, 2019</p>
  </a>
</div>

            
          
            
              

            
          
        </div>
        
          <hr>
        
      </div>
    
  </section>


      <section class="article typo">
        <div class="article-entry" itemprop="articleBody">
          
          
          <p>Attention机制简单总结</p>
<a id="more"></a>

<h3 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h3><ul>
<li><p>按照许多介绍Attention机制的博客的惯例，都会以图像领域的例子作为引子，这里也仿照诸位前辈的套路，对Attention做感性的认识。</p>
</li>
<li><p>实际上，Attention机制刚开始就是应用在图像领域中，并且取得了很好的效果，后来就有人尝试将Attention引入NLP领域中</p>
</li>
<li><p>以人的注意力机制为例，当我们看到一幅画的时候，我们虽然能够看到画的全貌，但是在深入观察的时候，眼睛真正聚焦的只是其中的画的一部分，也就是说，人脑对整幅画面的关注程度是不均衡的。用更形式话的语言来说就是——对于画面的不同局部，在人脑中成像时所占的权重是不同的。这种权重的思想就是AM的核心。</p>
</li>
<li><p>Attention Model在NLP中最早应用的领域是机器翻译。简单来说就是在Decode的时候，由于不同语言语法或者表述上的不同，每个时刻翻译的输出对应输入序列的不同局部，并且并不是顺序对应的，这时候利用Attention机制就很容易能够完成这项工作了。</p>
</li>
<li><p>Attention机制在NLP领域中有很广泛的应用，并且出现了很多变体，这里会尽可能全面的去介绍一下。本人才疏学浅，有遗漏或者表述错误的地方烦请指正。</p>
</li>
<li><p>如有侵权，请联系删除或注明</p>
</li>
</ul>
<h3 id="Attention的一个通用的定义"><a href="#Attention的一个通用的定义" class="headerlink" title="Attention的一个通用的定义"></a>Attention的一个通用的定义</h3><ul>
<li>给定一组向量集合values，以及一个向量query，attention机制就是一种根据query计算values的加权求和的机制</li>
<li>Attention的重点就是每个value的权值的计算。相对应的，权值大的就意味着关注程度高，对于当前输出有更高的作用。</li>
<li>换言之，Attention机制是根据一些规则或者附加信息，对向量集合中抽取向量进行加权求和。</li>
<li><del>更简单粗暴一点的讲，**只要我们从向量中选取部分向量进行的加权求和，我们就使用的Attention</del>**</li>
<li>更正：Attention与求和没有关系，重点是权重分配，也就是部分向量对于当前任务是积极的还是消极的</li>
</ul>
<h3 id="翻译任务中的Attention"><a href="#翻译任务中的Attention" class="headerlink" title="翻译任务中的Attention"></a>翻译任务中的Attention</h3><ul>
<li><p>核心思想：目标语言的每一个字应该是它在源语言中每个字的不同权重的加权和</p>
</li>
<li><p>在Encoder-Decoder结构中，Encoder会把所有的输入序列编码成统一的语义特征C在进行解码，因此，C中必须包含原始序列的全部信息，而它的长度就限制了存储信息的能力，也就限制了翻译的质量。</p>
</li>
<li><p>Attention机制在其中的应用方式就是，在每个时刻输入不同的C（不同的加权和）来解决只有一个C时<strong>信息存储能力不足</strong>的问题</p>
<p><img src="/.top//Attention%E6%9C%BA%E5%88%B6%E7%AE%80%E5%8D%95%E6%80%BB%E7%BB%93%5Cattention1.jpg" alt="attention1"></p>
</li>
<li><blockquote>
<p>对于上图来说，就是在Encoder和hidden之间了一层Attention，每一个$c_i$都会自动选取与当前索要输出的y最和是的上下文信息</p>
<p>而如何来选择合适的上下文信息呢，这就直接与权重（姑且以$a_{ij}$来表示）相关了，而权重是通过模型训练得到的</p>
</blockquote>
<p><img src="/.top//Attention%E6%9C%BA%E5%88%B6%E7%AE%80%E5%8D%95%E6%80%BB%E7%BB%93%5Cattention2.png" alt="attention2"></p>
</li>
</ul>
<ul>
<li><blockquote>
<p>对于机翻的Attention机制，我们做更进一步的解释：</p>
<ol>
<li>在Encoder的过程中保留每个隐藏状态（$h_1, … , h_n$）</li>
<li>对于Decoder的每一个时刻，此时能得到Decoder的输入和上一步的隐藏状态输出，所以可以得到当前步的隐藏状态，记为$S_t$</li>
<li>在每一步进行$S_t \cdot h_i$的到attention score —— 对应图中第二层</li>
<li>利用Softmax将attention score转化为概率分布（和为的概率） —— 对应第三行</li>
<li>按照得到的概率分布，计算Encoder和hidden states的加权和。这里得到的$Attention_t$就是Decoder在t时刻的注意力向量了</li>
</ol>
</blockquote>
</li>
</ul>
<h3 id="Attention机制的诸多变体"><a href="#Attention机制的诸多变体" class="headerlink" title="Attention机制的诸多变体"></a>Attention机制的诸多变体</h3><h4 id="Self-attention"><a href="#Self-attention" class="headerlink" title="Self-attention"></a>Self-attention</h4><ul>
<li><p>出自《Attention is all you need》，该文章还提出了tranformer模型，我会尝试在<a href="http://www.zivblog.top/passages/transformer-jian-dan-li-jie/" target="_blank" rel="noopener">另一篇文章</a>中简单介绍一下</p>
<p><img src="/.top//Attention%E6%9C%BA%E5%88%B6%E7%AE%80%E5%8D%95%E6%80%BB%E7%BB%93%5Cattention3.png" alt="attention3"></p>
</li>
</ul>
<blockquote>
<ol>
<li><p>序列中每一字符对其上下文字符的影响作用都不尽相同，每个字对序列的语义信息贡献也不同，可以通过一种机制将源输入序列中字符向量通过加权的方式，融合所有字符的语义向量信息来产生新的向量，从一定程度上增强了语义信息</p>
</li>
<li><p>对于输入文本，我们对其中的每个字分别增强语义向量表示：</p>
<ol>
<li>我们将每个字作为Query， 每一个文字由成对的&lt;key, value&gt;构成，它们都来自编码层</li>
</ol>
</li>
<li><p>Self-Attention = $Attention(X, X, X)$，也就是说再序列内部做attention，寻找序列内部的关系</p>
</li>
</ol>
</blockquote>
<blockquote>
<p><strong>2019.11.1 更新</strong></p>
<p>可以想到的是，在增加了self-attention之后，模型能更好的捕获句子长距离的相互依赖特征。因为self-attention是跨越距离直接将两个句子联系起来，而不是像RNN和LSTM那样通过会有遗忘的问题。</p>
</blockquote>
<blockquote>
<p><strong>2019.11.29 更新</strong></p>
<p><img src="/.top//Attention%E6%9C%BA%E5%88%B6%E7%AE%80%E5%8D%95%E6%80%BB%E7%BB%93%5Csf.png" alt="sf"></p>
<ul>
<li>对于每一个<code>self-attention</code>，均有独立维护的的三个线性映射矩阵$（W_i^V, W_i^K, W_i^Q）$</li>
<li>将输入的矩阵X与三个映射矩阵相乘，得到self-attention的三个输入Querys、Keys、Values</li>
<li>用Query和Keys的点积计算所有单词相对于当前词的得分score，该分数决定在编码该单词时其他单词给予了多少贡献</li>
<li>将score除以向量维度的平方根，在对其进行softmax归一化。这样对于每个单词都会获得所有单词对该单词编码的贡献分数，当然当前单词将获得最大分数，但也会关注其他单词的贡献大小</li>
<li>对于得到的socre，我们将其乘以每一个对应的value向量，即对所有向量加权求和，最后的输出就是self-attention对该向量的输出</li>
</ul>
</blockquote>
<ul>
<li>这里额外解释一下Query、Key、Value的传统含义</li>
</ul>
<blockquote>
<ol>
<li>Query从字面意思来讲，就是查询，查询“我与你们的相似度是多少？”</li>
<li>Key是键，即Query要比较的值，</li>
<li>Value是值，与Key一一对应，</li>
<li>Query与key相似度为a，即Value的权重（通过$Softmax(q_t \cdot k_s)$，得到$q_t$与$v_s$的相似度，再加权求和）</li>
</ol>
<p>当Query来自解码层，Key、Value来自编码层时，就是最基本的attention</p>
<p>当Query和Key、Value均来自编码层时，就是我们这里讲的Self-Attention了，因为它计算的事序列内部的依赖关系</p>
</blockquote>
<blockquote>
<p><strong>2019.10.23 更新</strong></p>
<p>self-attention计算得到的分数值，决定了我们在某位置encode一个词的时候，对句子其他部分的关注程度，比如一些代词与其真正实体间对应关系</p>
</blockquote>
<h4 id="Multi-head-Attention"><a href="#Multi-head-Attention" class="headerlink" title="Multi-head Attention"></a>Multi-head Attention</h4><ul>
<li><p>对于Scaled dot-product attention，只是除以了一个$\sqrt{d_k}$，以防止softmax进入梯度极小的区域</p>
<p>$$Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V$$</p>
</li>
<li><p>而Multi-head Attention是，从一个向量$q_t$出发</p>
<p><img src="/.top//Attention%E6%9C%BA%E5%88%B6%E7%AE%80%E5%8D%95%E6%80%BB%E7%BB%93%5Cattention4.png" alt></p>
<p>$$Attention(q_t, K, V) = \sum^m_{s=1} \frac{1}{z}exp(\frac{&lt;q_t, k_s&gt;}{\sqrt{s_k}})v_s$$</p>
<p>$$head_i = Attention(QW^Q_i, KW^K_i, VW^V_i)$$</p>
<p>$$MultiHead(Q, K, V) = Concat(head_1, …, head_h)$$</p>
</li>
<li><p>注意，这里并行计算的多个Scaled Dot-Product Attention之间的参数是不共享的</p>
</li>
<li><blockquote>
<p><strong>2019.11.29 更新</strong></p>
<ul>
<li>Linear可以看作是一个没有激活函数的全连接层，各自维护了一个线性映射矩阵</li>
<li><code>multi-head attention</code>即并行多个<code>self-attention</code></li>
<li>对于每个<code>self-attention</code>的输出进行简单的拼接，并与一个映射矩阵相乘，从而得到整体矩阵的输出</li>
</ul>
<p>参考：<a href="https://www.cnblogs.com/sandwichnlp/p/11612596.html" target="_blank" rel="noopener">https://www.cnblogs.com/sandwichnlp/p/11612596.html</a></p>
</blockquote>
</li>
</ul>
<ul>
<li>从两个方面提高了注意力层的性能：<ul>
<li>扩展了模型专注于不同位置的能力</li>
<li>给出了注意力层的多个表示子空间</li>
</ul>
</li>
</ul>
<h4 id="Soft-Attention"><a href="#Soft-Attention" class="headerlink" title="Soft Attention"></a>Soft Attention</h4><ul>
<li>所谓Soft，就是在求Attention的概率分布的时候，对于句子中的任意一个单词都给出一个概率</li>
<li>也就是说Soft是给每个单词都赋予了一个单词对齐概率</li>
</ul>
<h4 id="Hard-Attention"><a href="#Hard-Attention" class="headerlink" title="Hard Attention"></a>Hard Attention</h4><ul>
<li>与Soft Attention相对比来说，Hard Attention并没有为每个单词赋予一个对齐概率</li>
<li>它的做法是，直接从句子中找到某个特定的单词，然后把目标句子单词和这个单词堆砌，而其他输入句子中的单词硬性地规定其对齐概率为0</li>
<li>这种方式明显不够灵活，代价太高</li>
<li>并且已经被证明在图像领域可行，而在文本领域作用不大</li>
</ul>
<h4 id="Global-Attention"><a href="#Global-Attention" class="headerlink" title="Global Attention"></a>Global Attention</h4><ul>
<li>在解码过程中，每一时刻的上下文向量都需要计算Encoder中每一个单词（所谓的Global）的Attention权重，再加权得到</li>
</ul>
<h4 id="Local-Attention"><a href="#Local-Attention" class="headerlink" title="Local Attention"></a>Local Attention</h4><ul>
<li>首先找到一个对齐位置，在对齐位置左右大小固定的窗口内（所谓的Local）计算注意力权重，最终加权得到上下文向量</li>
</ul>
<h4 id="静态-Attention"><a href="#静态-Attention" class="headerlink" title="静态 Attention"></a>静态 Attention</h4><ul>
<li>对一个文档或者句子，计算每个词的Attention分布，然后加权得到一个向量来表示这个文档或者句子的向量表示</li>
<li>区别与Soft Attention，静态Attention只计算一个得到句子的向量表示</li>
</ul>
<h3 id="后面想到的"><a href="#后面想到的" class="headerlink" title="后面想到的"></a>后面想到的</h3><p><strong>2019.11.1更新</strong></p>
<p><img src="/.top//Attention%E6%9C%BA%E5%88%B6%E7%AE%80%E5%8D%95%E6%80%BB%E7%BB%93%5Cattention6.jpg" alt="attention6"></p>
<blockquote>
<p>浅析一下attention的本质思想：</p>
<p>将source中的构成元素想象成是由一系列&lt;key, value&gt;数据对组成</p>
<p>如上图所示，给定target中的某个元素的Query，通过计算Query与各个key（K）的相似性与相关性，得到每个key对应value的权重稀疏，然后对value进行甲醛求和，即最终的attention值</p>
<p>而对于attention计算的第一阶段，计算相似度的方式有多种，常见的有：点积、cosin相似性、MLP网络</p>
</blockquote>
<p><img src="/.top//Attention%E6%9C%BA%E5%88%B6%E7%AE%80%E5%8D%95%E6%80%BB%E7%BB%93%5Cattention5.png" alt="attention5"></p>
<blockquote>
<p>Attention的计算过程如上图所示，</p>
<ul>
<li>首先根据Q和K计算权重权重系数<ul>
<li>根据Q和K计算相似性</li>
<li>利用softmax进行归一化处理</li>
</ul>
</li>
<li>根据权重对value加权求和</li>
</ul>
</blockquote>
<h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><ul>
<li><a href="https://www.jianshu.com/p/6b2e586f9256" target="_blank" rel="noopener">https://www.jianshu.com/p/6b2e586f9256</a></li>
<li><a href="http://www.360doc.com/content/18/0506/06/36490684_751494854.shtml" target="_blank" rel="noopener">http://www.360doc.com/content/18/0506/06/36490684_751494854.shtml</a></li>
<li><a href="https://blog.csdn.net/hahajinbu/article/details/81940355" target="_blank" rel="noopener">https://blog.csdn.net/hahajinbu/article/details/81940355</a></li>
<li><a href="https://blog.csdn.net/mpk_no1/article/details/72862348" target="_blank" rel="noopener">https://blog.csdn.net/mpk_no1/article/details/72862348</a></li>
<li>《Attention is all you need》</li>
</ul>

          
            <div class='article_footer'>
              
                
  
    
    



  

  
    
    



  

  
    
    

<section class="widget copyright  desktop mobile">
  <div class='content'>
    
      <blockquote>
        
          
            <p>博客内容遵循 署名-非商业性使用-相同方式共享 4.0 国际 (CC BY-NC-SA 4.0) 协议</p>

          
        
          
            <p>本文永久链接是：<a href=http://zivblog.top/passages/attention-ji-zhi-jian-yao-zong-jie/>http://zivblog.top/passages/attention-ji-zhi-jian-yao-zong-jie/</a></p>
          
        
      </blockquote>
    
  </div>
</section>

  

  
    
    

<section class="widget qrcode  desktop mobile">
  

  <div class='content article-entry'>
    
      
        <div class='fancybox'><img src='https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/qrcode/wiki_volantis.png'
        
          height='64px'
        ></div>
      
    
      
        <div class='fancybox'><img src='https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/qrcode/wiki_volantis.png'
        
          height='64px'
        ></div>
      
    
  </div>
</section>

  


              
            </div>
          
        </div>
        
          


  <section class='meta' id="footer-meta">
    <div class='new-meta-box'>
      
        
          <div class="new-meta-item date" itemprop="dateUpdated" datetime="2020-07-06T13:05:56+08:00">
  <a class='notlink'>
    <i class="fas fa-edit fa-fw" aria-hidden="true"></i>
    <p>更新于：Jul 6, 2020</p>
  </a>
</div>

        
      
        
          
  
  <div class="new-meta-item meta-tags"><a class="tag" href="/tags/Artificial-Intelligence/" rel="nofollow"><i class="fas fa-hashtag fa-fw" aria-hidden="true"></i><p>Artificial Intelligence</p></a></div>


        
      
        
          

        
      
        
          
  <div class="new-meta-item share -mob-share-list">
  <div class="-mob-share-list share-body">
    
      
        <a class="-mob-share-qq" title="" rel="external nofollow noopener noreferrer"
          
          href="http://connect.qq.com/widget/shareqq/index.html?url=http://zivblog.top/passages/attention-ji-zhi-jian-yao-zong-jie/&title=Attention机制简单总结 - Ziv（瓶子）&summary=Attention机制简单总结"
          
          >
          
            <img src="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/logo/128/qq.png">
          
        </a>
      
    
      
        <a class="-mob-share-qzone" title="" rel="external nofollow noopener noreferrer"
          
          href="https://sns.qzone.qq.com/cgi-bin/qzshare/cgi_qzshare_onekey?url=http://zivblog.top/passages/attention-ji-zhi-jian-yao-zong-jie/&title=Attention机制简单总结 - Ziv（瓶子）&summary=Attention机制简单总结"
          
          >
          
            <img src="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/logo/128/qzone.png">
          
        </a>
      
    
      
        <a class="-mob-share-weibo" title="" rel="external nofollow noopener noreferrer"
          
          href="http://service.weibo.com/share/share.php?url=http://zivblog.top/passages/attention-ji-zhi-jian-yao-zong-jie/&title=Attention机制简单总结 - Ziv（瓶子）&summary=Attention机制简单总结"
          
          >
          
            <img src="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/logo/128/weibo.png">
          
        </a>
      
    
  </div>
</div>



        
      
    </div>
  </section>


        
        
          <div class="prev-next">
            
              <a class='prev' href='/passages/transformer-jian-dan-li-jie/'>
                <p class='title'><i class="fas fa-chevron-left" aria-hidden="true"></i>Transformer简单理解</p>
                <p class='content'>《Attention is all you need》


背景
传统的序列转换模型都是基于包含Encoder和Decoder的复杂的循环神经网络
比较好的模型都是在Encoder和Decode...</p>
              </a>
            
            
              <a class='next' href='/passages/gru-de-jian-dan-li-jie-yu-shi-xian/'>
                <p class='title'>GRU的简单理解与实现<i class="fas fa-chevron-right" aria-hidden="true"></i></p>
                <p class='content'>GRU的简单理解与实现




写在前面
之前介绍了传统的RNN和LSTM，GRU可以看作是LSTM的一种变体，它相较于LSTM网络结构更加简单，因此训练起来更加快速。
GRU是LSTM的变体，...</p>
              </a>
            
          </div>
        
      </section>
    </article>
  

  
    <!-- 显示推荐文章和评论 -->



  <article class="post white-box reveal comments floatable">
    <section class="article typo">
      <p ct><i class='fas fa-comments'></i> 评论</p>
      
      
      
      
      
      
        <section id="comments">
          <div id="valine_container" class="valine_thread">
            <i class="fas fa-cog fa-spin fa-fw fa-2x"></i>
          </div>
        </section>
      
      
    </section>
  </article>


  




<!-- 根据页面mathjax变量决定是否加载MathJax数学公式js -->



  <script>
    window.subData = {
      title: 'Attention机制简单总结',
      tools: true
    }
  </script>


</div>
<aside class='l_side'>
  
  

  
    
    



  <section class="widget toc-wrapper shadow desktop mobile" id="toc-div" >
    
  <header>
    
      <i class="fas fa-list fa-fw" aria-hidden="true"></i><span class='name'>本文目录</span>
    
  </header>


    <div class='content'>
        <ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#写在前面"><span class="toc-text">写在前面</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Attention的一个通用的定义"><span class="toc-text">Attention的一个通用的定义</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#翻译任务中的Attention"><span class="toc-text">翻译任务中的Attention</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Attention机制的诸多变体"><span class="toc-text">Attention机制的诸多变体</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Self-attention"><span class="toc-text">Self-attention</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Multi-head-Attention"><span class="toc-text">Multi-head Attention</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Soft-Attention"><span class="toc-text">Soft Attention</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Hard-Attention"><span class="toc-text">Hard Attention</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Global-Attention"><span class="toc-text">Global Attention</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Local-Attention"><span class="toc-text">Local Attention</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#静态-Attention"><span class="toc-text">静态 Attention</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#后面想到的"><span class="toc-text">后面想到的</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#参考资料"><span class="toc-text">参考资料</span></a></li></ol>
    </div>
  </section>


  


</aside>


  
  <footer class="clearfix">
    <br><br>
    
      
        <div class="aplayer-container">
          

  
    <meting-js
      theme='#1BCDFC'
      autoplay='false'
      volume='0.7'
      loop='all'
      order='list'
      fixed='false'
      list-max-height='340px'
      server='netease'
      type='playlist'
      id='3175833810'
      list-folded='true'>
    </meting-js>
  


        </div>
      
    
      
        <br>
        <div class="social-wrapper">
          
            
              <a href="/atom.xml"
                class="social fas fa-rss flat-btn"
                target="_blank"
                rel="external nofollow noopener noreferrer">
              </a>
            
          
            
              <a href="mailto:jfwang9@stu.suda.edu.cn"
                class="social fas fa-envelope flat-btn"
                target="_blank"
                rel="external nofollow noopener noreferrer">
              </a>
            
          
            
              <a href="https://github.com/cn-Wziv"
                class="social fab fa-github flat-btn"
                target="_blank"
                rel="external nofollow noopener noreferrer">
              </a>
            
          
            
              <a href="https://music.163.com/#/user/home?id=63035382"
                class="social fas fa-headphones-alt flat-btn"
                target="_blank"
                rel="external nofollow noopener noreferrer">
              </a>
            
          
        </div>
      
    
      
        <div><p>Blog content follows the <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en" target="_blank" rel="noopener">Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0) License</a></p>
</div>
      
    
      
        Use
        <a href="https://volantis.js.org/" target="_blank" class="codename">Ziv</a>
        as theme, total visits
          <span id="busuanzi_value_site_pv"><i class="fas fa-circle-notch fa-spin fa-fw" aria-hidden="true"></i></span>
          times
        
      
    
      
        <div class='copyright'>
        <p><a href="http://zivblog.top">Copyright © 2017-2020 Mr. Pingzi</a></p>

        </div>
      
    
  </footer>

<script>setLoadingBarProgress(80);</script>


      <script>setLoadingBarProgress(60);</script>
    </div>
    <a class="s-top fas fa-arrow-up fa-fw" href='javascript:void(0)'></a>
  </div>
  
<script src="https://cdn.jsdelivr.net/npm/jquery@3.4/dist/jquery.min.js"></script>


  <script>
    
    var SEARCH_SERVICE = "hexo" || "hexo";
    var ROOT = "/" || "/";
    if (!ROOT.endsWith('/')) ROOT += '/';
  </script>





  <script async src="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-volantis@2/js/instant_page.js" type="module" defer integrity="sha384-OeDn4XE77tdHo8pGtE1apMPmAipjoxUQ++eeJa6EtJCfHlvijigWiJpD7VDPWXV1"></script>


  <script src="https://cdn.jsdelivr.net/npm/scrollreveal@4.0.6/dist/scrollreveal.min.js"></script>
  <script type="text/javascript">
    $(function() {
      ScrollReveal().reveal('.l_main .reveal', {
        distance: '8px',
        duration: '800',
        interval: '100',
        scale: '1'
      });
    });
  </script>


  
<script src="https://cdn.jsdelivr.net/npm/node-waves@0.7.6/dist/waves.min.js"></script>

  <script type="text/javascript">
    $(function() {
      Waves.attach('.flat-btn', ['waves-button']);
      Waves.attach('.float-btn', ['waves-button', 'waves-float']);
      Waves.attach('.float-btn-light', ['waves-button', 'waves-float', 'waves-light']);
      Waves.attach('.flat-box', ['waves-block']);
      Waves.attach('.float-box', ['waves-block', 'waves-float']);
      Waves.attach('.waves-image');
      Waves.init();
    });
  </script>


  <script defer src="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-busuanzi@2.3/js/busuanzi.pure.mini.js"></script>



  
  
  
    
<script src="https://cdn.jsdelivr.net/npm/jquery-backstretch@2.1.18/jquery.backstretch.min.js"></script>

    <script type="text/javascript">
      $(function(){
        var imgs=["https://cdn.jsdelivr.net/gh/xaoxuu/cdn-wallpaper/abstract/41F215B9-261F-48B4-80B5-4E86E165259E.jpeg", "https://cdn.jsdelivr.net/gh/xaoxuu/cdn-wallpaper/abstract/BBC19066-E176-47C2-9D22-48C81EE5DF6B.jpeg", "https://cdn.jsdelivr.net/gh/xaoxuu/cdn-wallpaper/abstract/B18FCBB3-67FD-48CC-B4F3-457BA145F17A.jpeg", "https://cdn.jsdelivr.net/gh/xaoxuu/cdn-wallpaper/abstract/35F12181-F0E9-45BD-B134-37E4B4A660CF.jpeg", "https://cdn.jsdelivr.net/gh/xaoxuu/cdn-wallpaper/abstract/00E0F0ED-9F1C-407A-9AA6-545649D919F4.jpeg", "https://cdn.jsdelivr.net/gh/xaoxuu/cdn-wallpaper/abstract/67239FBB-E15D-4F4F-8EE8-0F1C9F3C4E7C.jpeg", "https://cdn.jsdelivr.net/gh/xaoxuu/cdn-wallpaper/abstract/B951AE18-D431-417F-B3FE-A382403FF21B.jpeg", "https://cdn.jsdelivr.net/gh/xaoxuu/cdn-wallpaper/landscape/AEB33F9D-7294-4CF1-B8C5-3020748A9D45.jpeg", "https://cdn.jsdelivr.net/gh/xaoxuu/cdn-wallpaper/abstract/2884F904-F1F3-479E-AE27-5EBC291B63B0.jpeg", "https://cdn.jsdelivr.net/gh/xaoxuu/cdn-wallpaper/landscape/10A0FCE5-36A1-4AD0-8CF0-019259A89E03.jpeg", "https://cdn.jsdelivr.net/gh/xaoxuu/cdn-wallpaper/landscape/250662D4-5A21-4AAA-BB63-CD25CF97CFF1.jpeg", "https://cdn.jsdelivr.net/gh/xaoxuu/cdn-wallpaper/landscape/298468D7-E388-44A8-8CC5-8213BDC33CED.jpeg"];
        if ('true' == 'true') {
          function shuffle(arr){
            /*From countercurrent-time*/
            var n = arr.length;
            while(n--) {
              var index = Math.floor(Math.random() * n);
              var temp = arr[index];
              arr[index] = arr[n];
              arr[n] = temp;
            }
          }
          shuffle(imgs);
        }
        if ('') {
          $('').backstretch(
            imgs,
          {
            duration: "20000",
            fade: "1500"
          });
        } else {
          $.backstretch(
            imgs,
          {
            duration: "20000",
            fade: "1500"
          });
        }
      });
    </script>
  



  
    
<script src="https://cdn.jsdelivr.net/npm/aplayer@1.10/dist/APlayer.min.js"></script>

  
    
<script src="https://cdn.jsdelivr.net/npm/meting@2.0/dist/Meting.min.js"></script>

  









  
    
<script src="/js/valine.js"></script>

  
  <script>
  var GUEST_INFO = ['nick','mail','link'];
  var meta = 'nick,mail,link'.split(',').filter(function(item){
    return GUEST_INFO.indexOf(item) > -1
  });
  var REQUIRED_FIELDS = ['nick','mail','link'];
  var requiredFields = 'nick,mail'.split(',').filter(function(item){
    return REQUIRED_FIELDS.indexOf(item) > -1
  });
  var valine = new Valine();
  function emoji(path, idx, ext) {
      return path + "/" + path + "-" + idx + "." + ext;
  }
  var emojiMaps = {};
  for (var i = 1; i <= 54; i++) {
    emojiMaps['tieba-' + i] = emoji('tieba', i, 'png');
  }
  for (var i = 1; i <= 101; i++) {
    emojiMaps['qq-' + i] = emoji('qq', i, 'gif');
  }
  for (var i = 1; i <= 116; i++) {
    emojiMaps['aru-' + i] = emoji('aru', i, 'gif');
  }
  for (var i = 1; i <= 125; i++) {
    emojiMaps['twemoji-' + i] = emoji('twemoji', i, 'png');
  }
  for (var i = 1; i <= 4; i++) {
    emojiMaps['weibo-' + i] = emoji('weibo', i, 'png');
  }
  valine.init({
    el: '#valine_container',
    meta: meta,
    
    appId: "PwSLYbui2ApW0Rvu1hJu8ylp-gzGzoHsz",
    appKey: "H14C8D8IArWii8sXghM7JHfc",
    placeholder: "快来评论吧~",
    pageSize:'10',
    avatar:'robohash',
    lang:'zh-cn',
    visitor: 'true',
    highlight: 'true',
    mathJax: 'false',
    enableQQ: 'true',
    requiredFields: requiredFields,
    emojiCDN: 'https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/emoji/valine/',
    emojiMaps: emojiMaps
  })
  </script>





  
<script src="/js/app.js"></script>



  
<script src="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-volantis@2.6.5/js/search.js"></script>



  
<script src="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-volantis@2/js/comment_typing.js"></script>






<!-- 复制 -->

  <script src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js"></script>
<script>
  function wait(callback, seconds) {
    var timelag = null;
    timelag = window.setTimeout(callback, seconds);
  }
  !function (e, t, a) {
    var initCopyCode = function(){
      var copyHtml = '';
      copyHtml += '<button class="btn-copy" data-clipboard-snippet="">';
      copyHtml += '<i class="fas fa-copy"></i><span>COPY</span>';
      copyHtml += '</button>';
      $(".highlight .code pre").before(copyHtml);
      $(".article pre code").before(copyHtml);
      var clipboard = new ClipboardJS('.btn-copy', {
        target: function(trigger) {
          return trigger.nextElementSibling;
        }
      });
      clipboard.on('success', function(e) {
        let $btn = $(e.trigger);
        $btn.addClass('copied');
        let $icon = $($btn.find('i'));
        $icon.removeClass('fa-copy');
        $icon.addClass('fa-check-circle');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPIED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('fa-check-circle');
          $icon.addClass('fa-copy');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
      clipboard.on('error', function(e) {
        e.clearSelection();
        let $btn = $(e.trigger);
        $btn.addClass('copy-failed');
        let $icon = $($btn.find('i'));
        $icon.removeClass('fa-copy');
        $icon.addClass('fa-times-circle');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPY FAILED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('fa-times-circle');
          $icon.addClass('fa-copy');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
    }
    initCopyCode();
  }(window, document);
</script>




<!-- fancybox -->
<script src="https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script>
<script>
  function pjax_fancybox() {
    $(".article-entry").find("img").not('.inline').not('a img').each(function () { //渲染 fancybox
      var element = document.createElement("a"); // a 标签
      $(element).attr("pjax-fancybox", "");  // 过滤 pjax
      $(element).attr("href", $(this).attr("src"));
      if ($(this).attr("data-original")) {
        $(element).attr("href", $(this).attr("data-original"));
      }
      $(element).attr("data-fancybox", "images");
      var caption = "";   // 描述信息
      if ($(this).attr('alt')) {  // 标准 markdown 描述信息
        $(element).attr('data-caption', $(this).attr('alt'));
        caption = $(this).attr('alt');
      }
      var div = document.createElement("div");
      $(div).addClass("fancybox");
      $(this).wrap(div); // 最外层套 div ，其实主要作用还是 class 样式
      var span = document.createElement("span");
      $(span).addClass("image-caption");
      $(span).text(caption); // 加描述
      $(this).after(span);  // 再套一层描述
      $(this).wrap(element);  // 最后套 a 标签
    })
    $(".article-entry").find("img").fancybox({
      selector: '[data-fancybox="images"]',
      hash: false,
      loop: false,
      closeClick: true,
      helpers: {
        overlay: {closeClick: true}
      },
      buttons: [
        "zoom",
        "close"
      ]
    });
  };
  $(function () {
    pjax_fancybox();
  });
</script>





  <script>setLoadingBarProgress(100);</script>
</body>
</html>
